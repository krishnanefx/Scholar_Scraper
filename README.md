# ğŸ“ Scholar Scraper

```
  ____       _           _             ____                                   
 / ___|  ___| |__   ___ | | __ _ _ __  / ___|  ___ _ __ __ _ _ __   ___ _ __     
 \___ \ / __| '_ \ / _ \| |/ _` | '__| \___ \ / __| '__/ _` | '_ \ / _ \ '__|    
  ___) | (__| | | | (_) | | (_| | |     ___) | (__| | | (_| | |_) |  __/ |       
 |____/ \___|_| |_|\___/|_|\__,_|_|    |____/ \___|_|  \__,_| .__/ \___|_|       
                                                            |_|                  
```

ğŸš€ **Scholar Scraper** is a comprehensive Python-based tool designed to collect, process, and analyze academic profile data from major AI/ML conferences. The project includes advanced machine learning models for predicting conference participation, scholar profile matching, and interactive data exploration capabilities.

---

> ğŸ¯ **Goal**: Predict next-year conference participation (computed dynamically as latest-year-in-data + 1) with conservative, high-precision predictions
> 
> ğŸ“Š **Scale**: 46K+ scholar profiles, ~5.5K predictions across AAAI/NeurIPS/ICLR, comprehensive academic insights

---

## âœ¨ Features

### ğŸ”§ Core Capabilities
- **ğŸ•·ï¸ Scholar Profile Scraping**: Automated collection of Google Scholar profiles with Wikipedia enrichment
- **ğŸ“Š Conference Data Processing**: Aggregates participation data from AAAI, ICLR, and NeurIPS conferences
- **ğŸ¤– Machine Learning Predictions**: Advanced ensemble models predicting 2026 conference participation
- **ğŸ” Profile Matching**: Fuzzy string matching to link predicted participants with scholar profiles
- **ğŸ“ˆ Interactive Dashboard**: Streamlit-based visualization and exploration platform

### ğŸ§  Advanced Analytics
- **ğŸ¯ Participation Prediction Models**: Gradient Boosting + Logistic Regression ensemble with 5-fold cross-validation
- **âš™ï¸ Feature Engineering**: Temporal patterns, streak analysis, exponential decay scoring, Markov transitions
- **ğŸ›ï¸ Model Calibration**: Isotonic calibration with logic-based probability adjustments
- **ğŸ‘¥ Comprehensive Profiling**: Institution mapping, research interest analysis, citation metrics

## ğŸ—‚ï¸ Project Structure

### ğŸ“ Core Files
- `ğŸ“œ main.py`: Main script for scholar profile crawling and data processing
- `ğŸ“Š dashboard.py`: Streamlit dashboard for data visualization and exploration
- `ğŸ¯ conference_predictions_summary_2026.py`: Comprehensive 2026 participant prediction with profile matching

### ğŸ¤– Conference Prediction Models
- `AAAI scraper/predict_2026_authors.py`: AAAI 2026 participation prediction model
- `neurips_predict_2026_authors.py`: NeurIPS 2026 participation prediction model  
- `iclr_predict_2026_authors.py`: ICLR 2026 participation prediction model

### ğŸ’¾ Data Files
- `ğŸ‘¨â€ğŸ“ scholar_profiles_progressssss.csv`: Complete scholar profile database (46K+ profiles)
- `ğŸ“¦ iclr_2020_2025_combined_data.parquet`: ICLR conference participation data (2020-2025)
- `ğŸ“¦ neurips_2020_2024_combined_data.parquet`: NeurIPS conference participation data (2020-2024)
- `ğŸ“ˆ aaai_2026_predictions.csv`, `neurips_2026_predictions.csv`, `iclr_2026_predictions.csv`: 2026 predictions
- `ğŸ“Š 2026_Conference_Predictions_with_Scholar_Profiles.xlsx`: Comprehensive matched results

### âš™ï¸ Configuration & Cache
- `ğŸ“‹ queue.txt`: Profile crawling queue management
- `ğŸ’¾ wiki_lookup_cache.joblib`, `fuzzy_match_cache.json`: Performance optimization caches
- `ğŸ“ requirements.txt`: Python dependencies

## ğŸš€ Getting Started

### ğŸ“‹ Prerequisites
- ğŸ Python 3.8+
- ğŸŒ Chrome/Chromium browser (for web scraping)
- ğŸ“¦ Required Python packages (see `requirements.txt`)

### ğŸ’» Installation
1. **ğŸ“¥ Clone the repository:**
   ```bash
   git clone https://github.com/krishnanefx/Scholar_Scraper.git
   cd Scholar_Scraper
   ```

2. **âš¡ Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ¯ Usage

> ğŸ’¡ **Quick Start**: Follow these steps to get up and running in minutes!

#### ğŸ•·ï¸ Scholar Profile Scraping
```bash
ğŸš€ python main.py
```
```
âœ¨ What it does:
ğŸ”„ Crawls Google Scholar profiles using BFS traversal
ğŸ“š Enriches profiles with Wikipedia data
ğŸ” Performs fuzzy matching with conference participation data
```

#### ğŸ¤– Conference Participation Prediction (CLI / env-friendly)

The model scripts now accept CLI flags and environment variables to make runs reproducible and workspace-independent. Each script will compute the prediction year as (max year in the input data) + 1 by default.

Examples:

```bash
# Use the bundled defaults (paths resolved relative to the script):
python src/models/aaai_predict_authors.py
python src/models/neurips_predict_authors.py
python src/models/iclr_predict_authors.py

# Or pass explicit paths (CLI)
python src/models/iclr_predict_authors.py \
   --data-path data/raw/iclr_2020_2025.parquet \
   --output-dir data/predictions \
   --model-path data/processed/iclr_participation_model.pkl

# Or set environment variables (ICLR example)
export ICLR_DATA_PATH=data/raw/iclr_2020_2025.parquet
export ICLR_OUTPUT_DIR=data/predictions
export ICLR_MODEL_PATH=data/processed/iclr_participation_model.pkl
python src/models/iclr_predict_authors.py
```

If training produces many numerical warnings from scikit-learn (divide-by-zero/overflow), they are non-blocking â€” use `--quiet-warnings` to suppress them during training runs.

## ğŸ§ª Experiment runbook â€” how to run (copyable)

Quick commands to setup the environment and run model scripts reproducibly. Replace paths as needed.

1) Create & activate a virtualenv (macOS / zsh):

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2) Run model scripts (defaults resolve paths relative to script):

```bash
# Run ICLR prediction (auto computes prediction year = max_year + 1)
python src/models/iclr_predict_authors.py

# Run NeurIPS (example with explicit paths and quieter warnings)
python src/models/neurips_predict_authors.py \
   --data-path data/raw/neurips_2020_2024_combined_data.parquet \
   --output-dir data/predictions \
   --model-path data/processed/neurips_participation_model.pkl \
   --quiet-warnings

# Run AAAI
python src/models/aaai_predict_authors.py
```

3) Run using the venv's python explicitly (helpful in CI or scripts):

```bash
.venv/bin/python src/models/iclr_predict_authors.py --quiet-warnings
```

GPU / no-GPU guidance
- The prediction pipelines use scikit-learn (GradientBoostingClassifier + LogisticRegression) which run on CPU. No GPU required.
- If you plan to add GPU-accelerated training (e.g., LightGBM/XGBoost with GPU, or torch-based models), set CUDA and ensure GPU-enabled builds are installed. Example (bash):

```bash
export CUDA_VISIBLE_DEVICES=0
# then run your GPU-capable script (requires GPU-enabled packages)
python my_gpu_model.py
```

Reproducibility tips
- Pass explicit `--data-path` and `--model-path` when running experiments to keep outputs organized.
- Commit the repo and note the git SHA alongside any saved model files.


#### ğŸ“Š Comprehensive Analysis with Profile Matching
```bash
ğŸ¯ python conference_predictions_summary_2026.py
```
```
ï¿½ Output:
ğŸ“ˆ 91.4% match rate with scholar profiles
ï¿½ Comprehensive Excel report with full academic profiles
ğŸ“‹ Separate sheets for AAAI, NeurIPS, and ICLR predictions
```

#### ğŸ¨ Interactive Dashboard
```bash
ğŸŒŸ streamlit run dashboard.py
```
```
ğŸ”§ Features:
ğŸ” Explore scholar profiles and conference participation patterns
ğŸ“Š Visualize prediction results and academic metrics
ğŸ›ï¸ Interactive filtering and analysis tools
```

## ğŸ† Key Results (Summary)

The following consolidated results were generated from the historic conference datasets (AAAI, NeurIPS, ICLR). Predictions target the year after the last year available in each dataset (e.g., data to 2024 â†’ predictions for 2025). The process uses conservative thresholding (85th percentile on training probabilities) to favor precision over recall.

| Conference | Predictions (selected) | Matched to Scholar Profiles | Match Rate | Avg H-Index | Avg Citations |
|------------|------------------------:|---------------------------:|----------:|------------:|--------------:|
| AAAI       | 1,669                  | 1,540                      | 92.3%     | 38.1        | 13,433        |
| NeurIPS    | 2,194                  | 1,999                      | 91.1%     | 41.0        | 19,312        |
| ICLR       | 1,625                  | 1,479                      | 91.0%     | 43.2        | 22,737        |
| TOTAL      | **5,488**              | **5,018**                  | **91.4%** | 40.8        | 18,494        |

### ğŸ¤– Model Performance (detailed)
- Cross-validation AUC: ~0.76 - 0.85 across conferences (venue and dataset dependent)
- Cross-validation precision/recall/F1: conservative thresholding was chosen to keep precision high (~0.82-0.89) while maintaining moderate recall (~0.60-0.75) depending on conference and fold.
- Ensemble Method: VotingClassifier ensemble of Gradient Boosting (GBDT) and Logistic Regression, with isotonic calibration applied on the ensemble probabilities.
- Feature engineering highlights:
   - Lagged-year features: participation indicators per year (lagged to exclude target)
   - Streaks & gaps: consecutive-year streaks and gap statistics
   - Temporal recency: exponential decay sum for recent activity
   - Markov-like transitions: probability of participation given prior-year attendance
   - Rolling window trends: last-3-year rates and activity trend deltas

These features produced models that generalize well under GroupKFold cross-validation (grouped by author to avoid leakage).

### ğŸ… Top Performing Institutions
```
ğŸ¥‡ Google LLC          ğŸ¥ˆ Stanford University    ğŸ¥‰ Tsinghua University
ğŸ† MIT                 ğŸ† University of Toronto  ğŸ† Carnegie Mellon
```

## ğŸ—ï¸ Technical Architecture

### ğŸ¤– Machine Learning Pipeline
- **ğŸ“Š Data Processing**: Temporal feature engineering with participation patterns
- **ğŸ¯ Model Architecture**: VotingClassifier ensemble (Gradient Boosting + Logistic Regression)
- **ğŸ”„ Cross-Validation**: 5-fold GroupKFold to prevent data leakage
- **ğŸ“ˆ Calibration**: Isotonic calibration for probability estimation
- **ğŸ§  Logic Integration**: Domain-specific probability adjustments

### ğŸ”— Data Integration
- **ğŸ‘¨â€ğŸ“ Scholar Profiles**: 46K+ profiles with Google Scholar and Wikipedia data
- **ğŸ“š Conference Data**: Historical participation from AAAI, ICLR, NeurIPS (2020-2025)
- **ğŸ” Fuzzy Matching**: 80% similarity threshold for name matching
- **âœ¨ Profile Enrichment**: Institution mapping, research interest analysis, citation metrics

### âš¡ Performance Optimization
- **ğŸ’¾ Caching Systems**: Wikipedia lookup and fuzzy match caching
- **âš™ï¸ Batch Processing**: Efficient handling of large-scale predictions
- **ğŸ“‹ Progress Tracking**: Queue management and incremental processing

## ğŸ“¤ Output Formats

### ğŸ“Š Excel Reports
- **ğŸ“ˆ 2026_Conference_Predictions_with_Scholar_Profiles.xlsx**: Comprehensive matched results
- ğŸ“‹ Individual sheets per conference with full academic profiles
- ğŸ“Š Summary statistics and unmatched entries for manual review

### ğŸ“„ CSV Files
- ğŸ“ˆ Individual prediction files for each conference
- ğŸ‘¨â€ğŸ“ Scholar profile database with enriched metadata
- ğŸ’¾ Cached matching results for performance optimization

## ğŸ“¦ Requirements
- ğŸ Python 3.8+
- ğŸŒ Chrome/Chromium (for scraping components)
- Run `pip install -r requirements.txt` to install dependencies. The `requirements.txt` contains core ML/data libs and optional tooling for scraping and dashboarding.

Notable additions for developers:
- `pytest` â€” used for unit tests added under `tests/`
- `joblib` â€” for model serialization (used by scripts)

## ğŸ¤ Contributing
ğŸ‰ Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.

### ğŸ”§ Development Setup
```bash
git clone https://github.com/krishnanefx/Scholar_Scraper.git
cd Scholar_Scraper
pip install -r requirements.txt
```

### ğŸ“‹ TODO
- [ ] ğŸŒ Add more conferences (ICML, ACL, EMNLP)
- [ ] ğŸ¨ Enhanced dashboard visualizations
- [ ] ğŸ“Š Real-time prediction updates
- [ ] ğŸ” Advanced search and filtering

---

## ğŸ“„ License
ğŸ“œ MIT License

## ğŸ‘¨â€ğŸ’» Author
**krishnan** (GitHub: [@krishnanefx](https://github.com/krishnanefx))

---

<div align="center">

### ğŸŒŸ Star this repo if you found it helpful! ğŸŒŸ

[![GitHub stars](https://img.shields.io/github/stars/krishnanefx/Scholar_Scraper?style=social)](https://github.com/krishnanefx/Scholar_Scraper)
[![GitHub forks](https://img.shields.io/github/forks/krishnanefx/Scholar_Scraper?style=social)](https://github.com/krishnanefx/Scholar_Scraper)

**Made with â¤ï¸ for the academic research community**

</div>
